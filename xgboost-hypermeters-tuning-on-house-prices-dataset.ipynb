{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mdistiakahmedkhan/xgboost-hypermeters-tuning-on-house-prices-dataset?scriptVersionId=105854449\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## We will be trying to predict house price with ensemble technique XGBoost.\n\nXGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons.  \nXGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques.\n\n**Why XGBoost is havily used in machine learning competetions?**\n\nXGBoost helps to reduce overfitting. XGBoost implements parallel processing and is faster than GBM . XGBoost also supports implementation on Hadoop. High Flexibility: XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model. Handling Missing Values: XGBoost has an in-built routine to handle missing values. Tree Pruning: XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain. Built-in Cross-Validation: XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.","metadata":{}},{"cell_type":"markdown","source":"**Lets get started!**","metadata":{}},{"cell_type":"markdown","source":"## Check out the data \nWe've been able to get some data from your neighbor for housing prices as a csv set, let's get our environment ready with the libraries we'll need and then import the data!\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Import Libaries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nimport os\nprint(os.listdir(\"../input/house-prices-advanced-regression-techniques\"))","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:25.581667Z","iopub.execute_input":"2022-09-16T17:47:25.582125Z","iopub.status.idle":"2022-09-16T17:47:26.281947Z","shell.execute_reply.started":"2022-09-16T17:47:25.582087Z","shell.execute_reply":"2022-09-16T17:47:26.280568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:26.284216Z","iopub.execute_input":"2022-09-16T17:47:26.285102Z","iopub.status.idle":"2022-09-16T17:47:26.365688Z","shell.execute_reply.started":"2022-09-16T17:47:26.285047Z","shell.execute_reply":"2022-09-16T17:47:26.364708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:11.602037Z","iopub.execute_input":"2022-09-15T21:56:11.602663Z","iopub.status.idle":"2022-09-15T21:56:11.610858Z","shell.execute_reply.started":"2022-09-15T21:56:11.602629Z","shell.execute_reply":"2022-09-15T21:56:11.609549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:11.8725Z","iopub.execute_input":"2022-09-15T21:56:11.873337Z","iopub.status.idle":"2022-09-15T21:56:11.88291Z","shell.execute_reply.started":"2022-09-15T21:56:11.873286Z","shell.execute_reply":"2022-09-15T21:56:11.881312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:12.188294Z","iopub.execute_input":"2022-09-15T21:56:12.188742Z","iopub.status.idle":"2022-09-15T21:56:12.22054Z","shell.execute_reply.started":"2022-09-15T21:56:12.188707Z","shell.execute_reply":"2022-09-15T21:56:12.219263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:12.383152Z","iopub.execute_input":"2022-09-15T21:56:12.383584Z","iopub.status.idle":"2022-09-15T21:56:12.412945Z","shell.execute_reply.started":"2022-09-15T21:56:12.38355Z","shell.execute_reply":"2022-09-15T21:56:12.411415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns ","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:12.656973Z","iopub.execute_input":"2022-09-15T21:56:12.65743Z","iopub.status.idle":"2022-09-15T21:56:12.665591Z","shell.execute_reply.started":"2022-09-15T21:56:12.657395Z","shell.execute_reply":"2022-09-15T21:56:12.663951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:12.880367Z","iopub.execute_input":"2022-09-15T21:56:12.883242Z","iopub.status.idle":"2022-09-15T21:56:12.892461Z","shell.execute_reply.started":"2022-09-15T21:56:12.883201Z","shell.execute_reply":"2022-09-15T21:56:12.890631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What is the data trying to say to us ? We need to analyse the data. Analysing data is the most important thing to understand what the data is telling us.","metadata":{}},{"cell_type":"markdown","source":"Here's a brief version of what you'll find in the data description file.\n\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n\nMSSubClass: The building class\n\nMSZoning: The general zoning classification\n\nLotFrontage: Linear feet of street connected to property\n\nLotArea: Lot size in square feet\n\nStreet: Type of road access\n\nAlley: Type of alley access\n\nLotShape: General shape of property\n\nLandContour: Flatness of the property\n\nUtilities: Type of utilities available\n\nLotConfig: Lot configuration\n\nLandSlope: Slope of property\n\nNeighborhood: Physical locations within Ames city limits\n\nCondition1: Proximity to main road or railroad\n\nCondition2: Proximity to main road or railroad (if a second is \npresent)\n\nBldgType: Type of dwelling\n\nHouseStyle: Style of dwelling\n\nOverallQual: Overall material and finish quality\n\nOverallCond: Overall condition rating\n\nYearBuilt: Original construction date\n\nYearRemodAdd: Remodel date\n\nRoofStyle: Type of roof\n\nRoofMatl: Roof material\n\nExterior1st: Exterior covering on house\n\nExterior2nd: Exterior covering on house (if more than one \nmaterial)\n\nMasVnrType: Masonry veneer type\n\nMasVnrArea: Masonry veneer area in square feet\n\nExterQual: Exterior material quality\n\nExterCond: Present condition of the material on the exterior\n\nFoundation: Type of foundation\n\nBsmtQual: Height of the basement\n\nBsmtCond: General condition of the basement\n\nBsmtExposure: Walkout or garden level basement walls\n\nBsmtFinType1: Quality of basement finished area\n\nBsmtFinSF1: Type 1 finished square feet\n\nBsmtFinType2: Quality of second finished area (if present)\n\nBsmtFinSF2: Type 2 finished square feet\n\nBsmtUnfSF: Unfinished square feet of basement area\n\nTotalBsmtSF: Total square feet of basement area\n\nHeating: Type of heating\n\nHeatingQC: Heating quality and condition\n\nCentralAir: Central air conditioning\n\nElectrical: Electrical system\n\n1stFlrSF: First Floor square feet\n\n2ndFlrSF: Second floor square feet\n\nLowQualFinSF: Low quality finished square feet (all floors)\n\nGrLivArea: Above grade (ground) living area square feet\n\nBsmtFullBath: Basement full bathrooms\n\nBsmtHalfBath: Basement half bathrooms\n\nFullBath: Full bathrooms above grade\n\nHalfBath: Half baths above grade\n\nBedroom: Number of bedrooms above basement level\n\nKitchen: Number of kitchens\n\nKitchenQual: Kitchen quality\n\nTotRmsAbvGrd: Total rooms above grade (does not include \nbathrooms)\n\nFunctional: Home functionality rating\n\nFireplaces: Number of fireplaces\n\nFireplaceQu: Fireplace quality\n\nGarageType: Garage location\n\nGarageYrBlt: Year garage was built\n\nGarageFinish: Interior finish of the garage\n\nGarageCars: Size of garage in car capacity\n\nGarageArea: Size of garage in square feet\n\nGarageQual: Garage quality\n\nGarageCond: Garage condition\n\nPavedDrive: Paved driveway\n\nWoodDeckSF: Wood deck area in square feet\n\nOpenPorchSF: Open porch area in square feet\n\nEnclosedPorch: Enclosed porch area in square feet\n\n3SsnPorch: Three season porch area in square feet\n\nScreenPorch: Screen porch area in square feet\n\nPoolArea: Pool area in square feet\n\nPoolQC: Pool quality\n\nFence: Fence quality\n\nMiscFeature: Miscellaneous feature not covered in other \ncategories\n\nMiscVal: $Value of miscellaneous feature\n\nMoSold: Month Sold\n\nYrSold: Year Sold\n\nSaleType: Type of sale\n\nSaleCondition: Condition of sale","metadata":{}},{"cell_type":"markdown","source":"## Data Visualization \n\nLet's look at the point of visualization.","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:13.609365Z","iopub.execute_input":"2022-09-15T21:56:13.610526Z","iopub.status.idle":"2022-09-15T21:56:14.286046Z","shell.execute_reply.started":"2022-09-15T21:56:13.610479Z","shell.execute_reply":"2022-09-15T21:56:14.284725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see the sale price value is right skewed. We need to make this normal distributed","metadata":{}},{"cell_type":"markdown","source":"# Feature selection","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.heatmap(train.corr(),cmap='coolwarm',annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:37.427788Z","iopub.execute_input":"2022-09-16T17:47:37.428221Z","iopub.status.idle":"2022-09-16T17:47:43.393299Z","shell.execute_reply.started":"2022-09-16T17:47:37.428186Z","shell.execute_reply":"2022-09-16T17:47:43.392381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see the most corelated parameters in numerical values above plotting. And we can pick these as features for our macine learning model.","metadata":{}},{"cell_type":"code","source":"corr = train.corr()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:43.395204Z","iopub.execute_input":"2022-09-16T17:47:43.39587Z","iopub.status.idle":"2022-09-16T17:47:43.407555Z","shell.execute_reply.started":"2022-09-16T17:47:43.395812Z","shell.execute_reply":"2022-09-16T17:47:43.406561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr[corr['SalePrice']>0.2].index","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:43.409136Z","iopub.execute_input":"2022-09-16T17:47:43.409772Z","iopub.status.idle":"2022-09-16T17:47:43.428969Z","shell.execute_reply.started":"2022-09-16T17:47:43.409737Z","shell.execute_reply":"2022-09-16T17:47:43.427681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Heat map for features that is greater or equal to 0.2 of correlation of Sale Prices.","metadata":{}},{"cell_type":"code","source":"train=train[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'YearRemodAdd',\n       'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n       'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea',\n       'WoodDeckSF', 'OpenPorchSF', 'SalePrice']]\ntest_id=test['Id']\ntest=test[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'YearRemodAdd',\n       'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n       'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea',\n       'WoodDeckSF', 'OpenPorchSF']]","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:43.431201Z","iopub.execute_input":"2022-09-16T17:47:43.431581Z","iopub.status.idle":"2022-09-16T17:47:43.444669Z","shell.execute_reply.started":"2022-09-16T17:47:43.431536Z","shell.execute_reply":"2022-09-16T17:47:43.443783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We droped some columns that less than 0.2 of correlation of Sale Prices.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.heatmap(train.corr(),cmap='coolwarm',annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:42:56.880967Z","iopub.execute_input":"2022-09-16T15:42:56.882134Z","iopub.status.idle":"2022-09-16T15:42:59.373291Z","shell.execute_reply.started":"2022-09-16T15:42:56.882092Z","shell.execute_reply":"2022-09-16T15:42:59.371717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:24.841219Z","iopub.execute_input":"2022-09-15T21:56:24.841777Z","iopub.status.idle":"2022-09-15T21:56:24.85897Z","shell.execute_reply.started":"2022-09-15T21:56:24.841742Z","shell.execute_reply":"2022-09-15T21:56:24.857445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='1stFlrSF',y='SalePrice',data=train) # 1stFlrSF seems very corelated with SalePrice.","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:24.86118Z","iopub.execute_input":"2022-09-15T21:56:24.86207Z","iopub.status.idle":"2022-09-15T21:56:25.474527Z","shell.execute_reply.started":"2022-09-15T21:56:24.862033Z","shell.execute_reply":"2022-09-15T21:56:25.473191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x= 'GrLivArea', y='SalePrice', data = train)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:25.476051Z","iopub.execute_input":"2022-09-15T21:56:25.476385Z","iopub.status.idle":"2022-09-15T21:56:25.714151Z","shell.execute_reply.started":"2022-09-15T21:56:25.476353Z","shell.execute_reply":"2022-09-15T21:56:25.71272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.boxplot(x='GarageCars',y='SalePrice',data=train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:25.717149Z","iopub.execute_input":"2022-09-15T21:56:25.717672Z","iopub.status.idle":"2022-09-15T21:56:26.027585Z","shell.execute_reply.started":"2022-09-15T21:56:25.717602Z","shell.execute_reply":"2022-09-15T21:56:26.026217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='OverallQual',y='SalePrice',data=train)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:26.031447Z","iopub.execute_input":"2022-09-15T21:56:26.031834Z","iopub.status.idle":"2022-09-15T21:56:26.610898Z","shell.execute_reply.started":"2022-09-15T21:56:26.0318Z","shell.execute_reply":"2022-09-15T21:56:26.609985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='GarageArea',y='SalePrice',data=train)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:26.612479Z","iopub.execute_input":"2022-09-15T21:56:26.61313Z","iopub.status.idle":"2022-09-15T21:56:27.230457Z","shell.execute_reply.started":"2022-09-15T21:56:26.613096Z","shell.execute_reply":"2022-09-15T21:56:27.228971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.barplot(x='FullBath',y = 'SalePrice',data=train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:56:27.232073Z","iopub.execute_input":"2022-09-15T21:56:27.23314Z","iopub.status.idle":"2022-09-15T21:56:27.645717Z","shell.execute_reply.started":"2022-09-15T21:56:27.233106Z","shell.execute_reply":"2022-09-15T21:56:27.644248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n1. handle missing data\n2. deal with catagorical features\n3. select features most correlated with SalePrice","metadata":{}},{"cell_type":"code","source":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:50.918053Z","iopub.execute_input":"2022-09-16T17:47:50.918506Z","iopub.status.idle":"2022-09-16T17:47:50.944236Z","shell.execute_reply.started":"2022-09-16T17:47:50.918467Z","shell.execute_reply":"2022-09-16T17:47:50.942973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dealing With missing values**\n\nimputing values missing in the columns on train dataset","metadata":{}},{"cell_type":"code","source":"train_c=train.copy()\ntest_c=test.copy()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:55.353693Z","iopub.execute_input":"2022-09-16T17:47:55.354908Z","iopub.status.idle":"2022-09-16T17:47:55.3609Z","shell.execute_reply.started":"2022-09-16T17:47:55.354838Z","shell.execute_reply":"2022-09-16T17:47:55.359709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Score From Approach 2( imputation):Next, we use SimpleImputer to replace missing values with the mean value along each column.\nfrom sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nfor col in (train_c.columns):\n    train_c[col] = my_imputer.fit_transform(train_c[col].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:47:55.813006Z","iopub.execute_input":"2022-09-16T17:47:55.81344Z","iopub.status.idle":"2022-09-16T17:47:56.131613Z","shell.execute_reply.started":"2022-09-16T17:47:55.813407Z","shell.execute_reply":"2022-09-16T17:47:56.130385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#missing data\ntrain_c.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:02.019167Z","iopub.execute_input":"2022-09-16T17:48:02.019592Z","iopub.status.idle":"2022-09-16T17:48:02.034017Z","shell.execute_reply.started":"2022-09-16T17:48:02.019557Z","shell.execute_reply":"2022-09-16T17:48:02.032665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to do same thing to the test data","metadata":{}},{"cell_type":"code","source":"#missing data\ntotal_test = test.isnull().sum().sort_values(ascending=False)\npercent_test =(test_c.isnull().sum()/test_c .isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total_test, percent_test], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:03.195239Z","iopub.execute_input":"2022-09-16T17:48:03.195696Z","iopub.status.idle":"2022-09-16T17:48:03.222057Z","shell.execute_reply.started":"2022-09-16T17:48:03.195661Z","shell.execute_reply":"2022-09-16T17:48:03.221087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputation\nmy_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nfor col in (test_c.columns):\n    test_c[col] = my_imputer.fit_transform(test_c[col].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:04.065614Z","iopub.execute_input":"2022-09-16T17:48:04.066062Z","iopub.status.idle":"2022-09-16T17:48:04.094549Z","shell.execute_reply.started":"2022-09-16T17:48:04.066025Z","shell.execute_reply":"2022-09-16T17:48:04.09333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#missing data\ntest_c.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:04.729649Z","iopub.execute_input":"2022-09-16T17:48:04.730079Z","iopub.status.idle":"2022-09-16T17:48:04.742023Z","shell.execute_reply.started":"2022-09-16T17:48:04.730046Z","shell.execute_reply":"2022-09-16T17:48:04.740774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now there is no missing values in both test and training data.","metadata":{}},{"cell_type":"code","source":"train_c.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:09.5035Z","iopub.execute_input":"2022-09-16T17:48:09.503964Z","iopub.status.idle":"2022-09-16T17:48:09.542402Z","shell.execute_reply.started":"2022-09-16T17:48:09.503925Z","shell.execute_reply":"2022-09-16T17:48:09.541222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_c.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:10.040477Z","iopub.execute_input":"2022-09-16T17:48:10.040935Z","iopub.status.idle":"2022-09-16T17:48:10.048788Z","shell.execute_reply.started":"2022-09-16T17:48:10.040879Z","shell.execute_reply":"2022-09-16T17:48:10.047587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_c.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:12.087956Z","iopub.execute_input":"2022-09-16T17:48:12.088352Z","iopub.status.idle":"2022-09-16T17:48:12.123102Z","shell.execute_reply.started":"2022-09-16T17:48:12.088322Z","shell.execute_reply":"2022-09-16T17:48:12.121915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_c.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:12.536105Z","iopub.execute_input":"2022-09-16T17:48:12.53653Z","iopub.status.idle":"2022-09-16T17:48:12.544471Z","shell.execute_reply.started":"2022-09-16T17:48:12.536495Z","shell.execute_reply":"2022-09-16T17:48:12.543143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with catagorical values","metadata":{}},{"cell_type":"code","source":"numerical_cols = [cname for cname in test.columns if test[cname].dtype in ['int64', 'float64']]\n# Get list of categorical variables\nobject_cols = [cname for cname in test.columns if test[cname].dtype in ['object']]\nprint(\"Numerical columns:\")\nprint(numerical_cols)\nprint(\"\\n Object columns:\")\nprint(object_cols)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T22:13:22.287274Z","iopub.execute_input":"2022-09-15T22:13:22.287693Z","iopub.status.idle":"2022-09-15T22:13:22.297416Z","shell.execute_reply.started":"2022-09-15T22:13:22.28766Z","shell.execute_reply":"2022-09-15T22:13:22.29609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**no object columns remained after data cleaning step.**","metadata":{}},{"cell_type":"code","source":"train=train_c.copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:17.440759Z","iopub.execute_input":"2022-09-16T17:48:17.44119Z","iopub.status.idle":"2022-09-16T17:48:17.448255Z","shell.execute_reply.started":"2022-09-16T17:48:17.441157Z","shell.execute_reply":"2022-09-16T17:48:17.447036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=test_c.copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:17.815313Z","iopub.execute_input":"2022-09-16T17:48:17.816433Z","iopub.status.idle":"2022-09-16T17:48:17.82248Z","shell.execute_reply.started":"2022-09-16T17:48:17.81637Z","shell.execute_reply":"2022-09-16T17:48:17.821161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to pick some features for the model. For this we are going to use correlation matrix and we are going to pick most correlated with sale price.","metadata":{}},{"cell_type":"markdown","source":"**Train test split**","metadata":{}},{"cell_type":"code","source":"#Importing packages\nfrom sklearn.model_selection import train_test_split\n\nX = train.drop(columns=\"SalePrice\")\ny = train[\"SalePrice\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:22.601772Z","iopub.execute_input":"2022-09-16T17:48:22.602997Z","iopub.status.idle":"2022-09-16T17:48:22.611517Z","shell.execute_reply.started":"2022-09-16T17:48:22.60295Z","shell.execute_reply":"2022-09-16T17:48:22.610335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:23.007303Z","iopub.execute_input":"2022-09-16T17:48:23.009995Z","iopub.status.idle":"2022-09-16T17:48:23.020867Z","shell.execute_reply.started":"2022-09-16T17:48:23.009945Z","shell.execute_reply":"2022-09-16T17:48:23.019734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Parameter Tuning\n\n**Parameters**\nnthread:\nThis is used for parallel processing and the number of cores in the system should be entered..\nIf you wish to run on all cores, do not input this value. The algorithm will detect it automatically.\n\neta:\nAnalogous to learning rate in GBM.\nMakes the model more robust by shrinking the weights on each step.\n\nmin_child_weight:\nDefines the minimum sum of weights of all observations required in a child.\nUsed to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n\nmax_depth:\nIt is used to define the maximum depth.\nHigher depth will allow the model to learn relations very specific to a particular sample.\n\nmax_leaf_nodes:\nThe maximum number of terminal nodes or leaves in a tree.\nCan be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\nIf this is defined, GBM will ignore max_depth.\n\ngamma :\nA node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\nMakes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n\nsubsample:\nSame as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\nLower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n\ncolsample_bytree:\nIt is similar to max_features in GBM.\nDenotes the fraction of columns to be randomly sampled for each tree.","metadata":{}},{"cell_type":"markdown","source":"**Explanation of relevant parameters for this kernel.**\n\nbooster: Select the type of model to run at each iteration\ngbtree: tree-based models\ngblinear: linear models\nnthread: default to maximum number of threads available if not set\nobjective: This defines the loss function to be minimized\nParameters for controlling speed\n\nsubsample: Denotes the fraction of observations to be randomly samples for each tree\ncolsample_bytree: Subsample ratio of columns when constructing each tree.\nn_estimators: Number of trees to fit.\nImportant parameters which control overfiting\n\nlearning_rate: Makes the model more robust by shrinking the weights on each step\nmax_depth: The maximum depth of a tree.\nmin_child_weight: Defines the minimum sum of weights of all observations required in a child.","metadata":{}},{"cell_type":"markdown","source":"**General Approach**\n\n1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n\n2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n\n3. Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\nLower the learning rate and decide the optimal parameters .","metadata":{}},{"cell_type":"code","source":"#Importing Packages\n\nfrom xgboost import XGBRegressor , plot_importance\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:19:46.730754Z","iopub.execute_input":"2022-09-16T21:19:46.731468Z","iopub.status.idle":"2022-09-16T21:19:46.740262Z","shell.execute_reply.started":"2022-09-16T21:19:46.731415Z","shell.execute_reply":"2022-09-16T21:19:46.738418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning the hyper-parameters\n**GridSearchCV params:**\n\nestimator: estimator object\nparam_grid : dict or list of dictionaries\nscoring: A single string or a callable to evaluate the predictions on the test set. If None, the estimator’s score method is used.\nhttps://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\nn_jobs: Number of jobs to run in parallel. None means. -1 means using all processors.\ncv: cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold.","metadata":{}},{"cell_type":"code","source":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train,params):\n    param_tuning = params\n    xgb_model = XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                  \n                           scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n    print(gsearch.best_params_, gsearch.best_score_)\n\n\n\n    return ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:31.040989Z","iopub.execute_input":"2022-09-16T17:48:31.041896Z","iopub.status.idle":"2022-09-16T17:48:31.047908Z","shell.execute_reply.started":"2022-09-16T17:48:31.041854Z","shell.execute_reply":"2022-09-16T17:48:31.046916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step-1 Fix learning rate and number of estimators for tuning tree-based parameters**\n1. max_depth = 5 : This should be between 3-10. I’ve started with 5 but you can choose a different number as well. 4-6 can be good starting points.\n2. min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n3. gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\n4. subsample, colsample_bytree = 0.8 : This is a commonly used used start value. Typical values range between 0.5-0.9.\n5. scale_pos_weight = 1: Because of high class imbalance.\n","metadata":{}},{"cell_type":"code","source":"#Run only in the first run of the kernel.\nparams={\n    'learning_rate': [0.1],\n     'n_estimators': [200,400,600,800,1000],\n     'max_depth':[5],\n     'min_child_weight': [1],\n     'gamma':[0],\n     'subsample':[0.8],\n     'colsample_bytree':[0.8],\n     'objective': ['reg:squarederror'],\n     'nthread':[4],\n     'scale_pos_weight' :[1],\n     'seed':[27]\n }\n\nhyperParameterTuning(X_train, y_train,params)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T15:43:50.332925Z","iopub.execute_input":"2022-09-16T15:43:50.33436Z","iopub.status.idle":"2022-09-16T16:26:53.13107Z","shell.execute_reply.started":"2022-09-16T15:43:50.334305Z","shell.execute_reply":"2022-09-16T16:26:53.129899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here for n_esimators we got the value 200, which was the lowest value set by us.Optimum n_estimators value could be less than 200. for n_estimator we should check value lower than 200.","metadata":{}},{"cell_type":"code","source":"param_test2 = {\n 'n_estimators': range (55,200,5)\n}\ngsearch2 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=200, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch2.fit(X_train,y_train)\ngsearch2.best_params_, gsearch2.best_score_ ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T17:48:37.338452Z","iopub.execute_input":"2022-09-16T17:48:37.338907Z","iopub.status.idle":"2022-09-16T18:40:00.788919Z","shell.execute_reply.started":"2022-09-16T17:48:37.338871Z","shell.execute_reply":"2022-09-16T18:40:00.787628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 60 for n_estimators. ","metadata":{}},{"cell_type":"markdown","source":"**Step 2: Tune max_depth and min_child_weight**\n\n The ideal values are 1 for max_depth and 5 for min_child_weight. Lets go one step deeper and look for optimum values. We’ll search for values 1 above and below the optimum values because we took an interval of two.","metadata":{}},{"cell_type":"code","source":"param_test3 = {\n    'max_depth':range(3,10,1),\n    'min_child_weight': range(3,10,1)\n}\ngsearch3 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch3.fit(X_train,y_train)\ngsearch3.best_params_, gsearch3.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-09-16T18:47:09.531967Z","iopub.execute_input":"2022-09-16T18:47:09.53243Z","iopub.status.idle":"2022-09-16T19:31:29.534379Z","shell.execute_reply.started":"2022-09-16T18:47:09.53239Z","shell.execute_reply":"2022-09-16T19:31:29.53297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 6 for max_depth and 9 for min_child_weight. ","metadata":{}},{"cell_type":"markdown","source":"**Step 3 : Tune gamma**","metadata":{}},{"cell_type":"code","source":"param_test4 = {\n 'gamma':[i/10.0 for i in range(0,5)]\n}\ngsearch4 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch4.fit(X_train,y_train)\ngsearch4.best_params_, gsearch4.best_score_ ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T19:36:07.95993Z","iopub.execute_input":"2022-09-16T19:36:07.960401Z","iopub.status.idle":"2022-09-16T19:40:29.597741Z","shell.execute_reply.started":"2022-09-16T19:36:07.960359Z","shell.execute_reply":"2022-09-16T19:40:29.596765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 0 for gamma.","metadata":{}},{"cell_type":"markdown","source":"**Step-4 :Tune Subsample and colsample_bytree**","metadata":{}},{"cell_type":"code","source":"param_test5 = {\n 'subsample':[i/10.0 for i in range(4,10)],\n 'colsample_bytree':[i/10.0 for i in range(4,10)]\n}\ngsearch5 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test5, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch5.fit(X_train,y_train)\ngsearch5.best_params_, gsearch5.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-09-16T19:40:29.602758Z","iopub.execute_input":"2022-09-16T19:40:29.604482Z","iopub.status.idle":"2022-09-16T20:10:47.043564Z","shell.execute_reply.started":"2022-09-16T19:40:29.60443Z","shell.execute_reply":"2022-09-16T20:10:47.042542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 0.8 for colsample_bytree and 0.8 for subsample. Now we should try values in 0.05 interval around these.","metadata":{}},{"cell_type":"code","source":"param_test6 = {\n 'subsample':[i/100.0 for i in range(75,85,5)],\n 'colsample_bytree':[i/100.0 for i in range(75,85,5)]\n}\ngsearch6 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test6, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch6.fit(X_train,y_train)\ngsearch6.best_params_, gsearch6.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-09-16T20:20:39.330732Z","iopub.execute_input":"2022-09-16T20:20:39.331188Z","iopub.status.idle":"2022-09-16T20:24:16.102205Z","shell.execute_reply.started":"2022-09-16T20:20:39.331152Z","shell.execute_reply":"2022-09-16T20:24:16.101153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 0.75for colsample_bytree and 0.8 for subsample.","metadata":{}},{"cell_type":"markdown","source":"**Step 5: Tuning Regularization Parameters**","metadata":{}},{"cell_type":"code","source":"param_test7 = {\n 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n}\ngsearch7 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.75,\n objective= 'reg:squarederror', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test7, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch7.fit(X_train,y_train)\ngsearch7.best_params_, gsearch7.best_score_ ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T20:30:12.099346Z","iopub.execute_input":"2022-09-16T20:30:12.099752Z","iopub.status.idle":"2022-09-16T20:34:23.22871Z","shell.execute_reply.started":"2022-09-16T20:30:12.099716Z","shell.execute_reply":"2022-09-16T20:34:23.227515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 0.05 for reg_alpha.","metadata":{}},{"cell_type":"markdown","source":"**Step 6: Tuning Seed**","metadata":{}},{"cell_type":"code","source":"param_test8 = {\n  'seed':range(25,35,1)\n}\ngsearch8 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.75,\n objective= 'reg:squarederror', reg_alpha=0.05,nthread=1, scale_pos_weight=1,seed=27), \n param_grid = param_test8, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch8.fit(X_train,y_train)\ngsearch8.best_params_, gsearch8.best_score_ ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T20:43:10.711112Z","iopub.execute_input":"2022-09-16T20:43:10.71161Z","iopub.status.idle":"2022-09-16T20:43:16.288536Z","shell.execute_reply.started":"2022-09-16T20:43:10.711565Z","shell.execute_reply":"2022-09-16T20:43:16.287301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 27 for seed.","metadata":{}},{"cell_type":"markdown","source":"**Step 7: Reducing Learning Rate & increasing m_estimators**","metadata":{}},{"cell_type":"code","source":"param_test9 = {\n  'learning_rate':[0.01,0.001],\n  'n_estimators':[1000,2000,500,5000]\n}\ngsearch9 = GridSearchCV(estimator = XGBRegressor( learning_rate=0.1, n_estimators=60, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.75,\n objective= 'reg:squarederror', reg_alpha=0.05,nthread=1, scale_pos_weight=1,seed=27), \n param_grid = param_test9, scoring='neg_mean_absolute_error',n_jobs=4, cv=5)\ngsearch9.fit(X_train,y_train)\ngsearch9.best_params_, gsearch9.best_score_ ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T20:48:40.109123Z","iopub.execute_input":"2022-09-16T20:48:40.109527Z","iopub.status.idle":"2022-09-16T20:51:03.873418Z","shell.execute_reply.started":"2022-09-16T20:48:40.109496Z","shell.execute_reply":"2022-09-16T20:51:03.872253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we get the optimum values as 0.01 for learning_rate and 500 for n_estimators.","metadata":{}},{"cell_type":"markdown","source":"# Train and fit model","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBRegressor( learning_rate=0.1, n_estimators=500, max_depth=6,\n min_child_weight=9, gamma=0, subsample=0.8, colsample_bytree=0.75,\n objective= 'reg:squarederror', reg_alpha=0.05,nthread=1, scale_pos_weight=1,seed=27)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:13:54.309522Z","iopub.execute_input":"2022-09-16T21:13:54.310001Z","iopub.status.idle":"2022-09-16T21:13:54.319028Z","shell.execute_reply.started":"2022-09-16T21:13:54.309962Z","shell.execute_reply":"2022-09-16T21:13:54.317019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:13:54.820257Z","iopub.execute_input":"2022-09-16T21:13:54.821161Z","iopub.status.idle":"2022-09-16T21:13:57.149493Z","shell.execute_reply.started":"2022-09-16T21:13:54.821113Z","shell.execute_reply":"2022-09-16T21:13:57.148282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=xgb_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:13:57.151637Z","iopub.execute_input":"2022-09-16T21:13:57.152503Z","iopub.status.idle":"2022-09-16T21:13:57.16957Z","shell.execute_reply.started":"2022-09-16T21:13:57.15245Z","shell.execute_reply":"2022-09-16T21:13:57.168204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:13:57.171404Z","iopub.execute_input":"2022-09-16T21:13:57.17217Z","iopub.status.idle":"2022-09-16T21:13:57.177241Z","shell.execute_reply.started":"2022-09-16T21:13:57.172129Z","shell.execute_reply":"2022-09-16T21:13:57.17621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    #Print model report:\n    print (\"\\nModel Report\")\n    print('MAE:', metrics.mean_absolute_error(y_test,predictions))\n    print('MSE:', metrics.mean_squared_error(y_test,predictions))             \n    # get importance\n    plot_importance(xgb_model)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:45:08.433115Z","iopub.execute_input":"2022-09-16T21:45:08.435349Z","iopub.status.idle":"2022-09-16T21:45:08.935302Z","shell.execute_reply.started":"2022-09-16T21:45:08.435283Z","shell.execute_reply":"2022-09-16T21:45:08.93373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# create submission file","metadata":{}},{"cell_type":"code","source":"predictions_test =xgb_model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:45:49.515739Z","iopub.execute_input":"2022-09-16T21:45:49.51617Z","iopub.status.idle":"2022-09-16T21:45:49.547507Z","shell.execute_reply.started":"2022-09-16T21:45:49.516137Z","shell.execute_reply":"2022-09-16T21:45:49.546331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'Id':test_id,'SalePrice':predictions_test})\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:45:51.946225Z","iopub.execute_input":"2022-09-16T21:45:51.947245Z","iopub.status.idle":"2022-09-16T21:45:51.967408Z","shell.execute_reply.started":"2022-09-16T21:45:51.947142Z","shell.execute_reply":"2022-09-16T21:45:51.965479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T21:46:02.769771Z","iopub.execute_input":"2022-09-16T21:46:02.770247Z","iopub.status.idle":"2022-09-16T21:46:02.786629Z","shell.execute_reply.started":"2022-09-16T21:46:02.770209Z","shell.execute_reply":"2022-09-16T21:46:02.784997Z"},"trusted":true},"execution_count":null,"outputs":[]}]}